{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kva4uKCHw0Fi"
   },
   "source": [
    "<a href=\"http://colab.research.google.com/github/dipanjanS/nlp_workshop_odsc19/blob/master/Module05%20-%20NLP%20Applications/Project07E%20-%20Text%20Classification%20with%20BERT%20Deep%20Transfer%20Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_a-R2xFgtyUU"
   },
   "source": [
    "# Text Classification with BERT - Deep Transfer Learning\n",
    "\n",
    "![](https://i.imgur.com/MFd6n82.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LbeQ0ZsowuZ"
   },
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "44WbTML-Z6JQ",
    "outputId": "6b2b8844-1dd7-497b-a5a2-664124f8b669"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /opt/app-root/lib/python3.9/site-packages (0.0.52)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /opt/app-root/lib/python3.9/site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: anyascii in /opt/app-root/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
      "Requirement already satisfied: pyahocorasick in /opt/app-root/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/app-root/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: bert-tensorflow in /opt/app-root/lib/python3.9/site-packages (1.0.4)\n",
      "Requirement already satisfied: six in /opt/app-root/lib/python3.9/site-packages (from bert-tensorflow) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/app-root/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /opt/app-root/lib/python3.9/site-packages (4.61.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/app-root/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tensorflow_hub in /opt/app-root/lib/python3.9/site-packages (0.12.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow_hub) (3.17.3)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow_hub) (1.21.0)\n",
      "Requirement already satisfied: six>=1.9 in /opt/app-root/lib/python3.9/site-packages (from protobuf>=3.8.0->tensorflow_hub) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/app-root/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "!pip install bert-tensorflow\n",
    "!pip install tqdm\n",
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/app-root/lib/python3.9/site-packages (4.11.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/app-root/lib/python3.9/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/app-root/lib/python3.9/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: sacremoses in /opt/app-root/lib/python3.9/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/app-root/lib/python3.9/site-packages (from transformers) (1.21.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in /opt/app-root/lib/python3.9/site-packages (from transformers) (0.0.19)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib/python3.9/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/app-root/lib/python3.9/site-packages (from transformers) (4.61.1)\n",
      "Requirement already satisfied: six in /opt/app-root/lib/python3.9/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /opt/app-root/lib/python3.9/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/app-root/lib/python3.9/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/app-root/lib/python3.9/site-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/app-root/lib/python3.9/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/app-root/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICsFrmtQo0ij"
   },
   "source": [
    "# Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "cXGxumq_Z6JT"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as tf_hub\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from bert.tokenization import FullTokenizer\n",
    "import tqdm\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# tf.logging.set_verbosity(tf.logging.INFO)\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMzS6N-xo3HW"
   },
   "source": [
    "# GPU Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "aCnPGZk5Z6JV",
    "outputId": "0abbd366-3133-4b98-cff5-95c29dc5f758"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "0.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf_hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "oZGV4XE3Z6JY",
    "outputId": "9f7deb9a-1a30-4380-f4ad-51c4c0b47f82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_gpu_available())\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "xWeT_CIZZ6Jb",
    "outputId": "367a62cd-4ec5-4216-8b5e-6b9db5a876b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: line 1: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4X7iDM4Vo5g7"
   },
   "source": [
    "# Load and View Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/IPython/core/interactiveshell.py:3169: DtypeWarning: Columns (5,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 555957 entries, 0 to 555956\n",
      "Data columns (total 18 columns):\n",
      " #   Column                        Non-Null Count   Dtype \n",
      "---  ------                        --------------   ----- \n",
      " 0   date_received                 555957 non-null  object\n",
      " 1   product                       555957 non-null  object\n",
      " 2   sub_product                   397635 non-null  object\n",
      " 3   issue                         555957 non-null  object\n",
      " 4   sub_issue                     212622 non-null  object\n",
      " 5   consumer_complaint_narrative  66806 non-null   object\n",
      " 6   company_public_response       85124 non-null   object\n",
      " 7   company                       555957 non-null  object\n",
      " 8   state                         551070 non-null  object\n",
      " 9   zipcode                       551452 non-null  object\n",
      " 10  tags                          77959 non-null   object\n",
      " 11  consumer_consent_provided     123458 non-null  object\n",
      " 12  submitted_via                 555957 non-null  object\n",
      " 13  date_sent_to_company          555957 non-null  object\n",
      " 14  company_response_to_consumer  555957 non-null  object\n",
      " 15  timely_response               555957 non-null  object\n",
      " 16  consumer_disputed?            555957 non-null  object\n",
      " 17  complaint_id                  555957 non-null  int64 \n",
      "dtypes: int64(1), object(17)\n",
      "memory usage: 76.3+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset1 = pd.read_csv(\"consumer_complaints.csv\")\n",
    "dataset1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[['consumer_complaint_narrative','product']]\n",
    "# dataset1 = pd.DataFrame(columns=['review', 'sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/pandas/core/frame.py:4441: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Credit reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Student loan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Debt collection</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  review         sentiment\n",
       "0    NaN          Mortgage\n",
       "1    NaN          Mortgage\n",
       "2    NaN  Credit reporting\n",
       "3    NaN      Student loan\n",
       "4    NaN   Debt collection"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = pd.DataFrame(columns=['review', 'sentiment'])\n",
    "dataset = dataset1[['consumer_complaint_narrative','product']]\n",
    "dataset.rename({'consumer_complaint_narrative': 'review', 'product': 'sentiment'}, axis=1, inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[pd.notnull(dataset['review'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape\n",
    "dataset = dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['sentiment'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(dataset['sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bank account or service',\n",
       " 'Consumer Loan',\n",
       " 'Credit card',\n",
       " 'Credit reporting',\n",
       " 'Debt collection',\n",
       " 'Money transfers',\n",
       " 'Mortgage',\n",
       " 'Other financial service',\n",
       " 'Payday loan',\n",
       " 'Prepaid card',\n",
       " 'Student loan']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['sentiment'] = le.transform(dataset['sentiment'])\n",
    "\n",
    "# list(le.inverse_transform([2, 2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XXXX has claimed I owe them {$27.00} for XXXX ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Due to inconsistencies in the amount owed that...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In XX/XX/XXXX my wages that I earned at my job...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have an open and current mortgage with Chase...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XXXX was submitted XX/XX/XXXX. At the time I s...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  XXXX has claimed I owe them {$27.00} for XXXX ...          4\n",
       "1  Due to inconsistencies in the amount owed that...          1\n",
       "2  In XX/XX/XXXX my wages that I earned at my job...          6\n",
       "3  I have an open and current mortgage with Chase...          6\n",
       "4  XXXX was submitted XX/XX/XXXX. At the time I s...          6"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66806"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "nyMDImuTZ6Je",
    "outputId": "6ca0243f-4d88-41a4-a4e0-b2ba0ff8466b"
   },
   "outputs": [],
   "source": [
    "# dataset = pd.read_csv('https://github.com/dipanjanS/nlp_workshop_dhs18/raw/master/Unit%2011%20-%20Sentiment%20Analysis%20-%20Unsupervised%20Learning/movie_reviews.csv.bz2', compression='bz2')\n",
    "# dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "iu_aqmqiZ6Jh",
    "outputId": "07cd2853-595d-4986-c5f7-e275c9f5d203"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset['sentiment'] = [1 if sentiment == 'positive' else 0 \n",
    "#                             for sentiment in dataset['sentiment'].values]\n",
    "# dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a8a053fcfd4845927a54aee1c147e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e616c73e254f709930cd3a12102e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5022cd6a7fc247e6bd2bac5e90feec0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336268aff2d8425bbfc464bc41169d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized ['[CLS]', 'test', 'token', '##ization', 'sentence', '.', 'followed', 'by', 'another', 'sentence', '[SEP]']\n",
      "{'token_ids': [101, 3231, 19204, 3989, 6251, 1012, 2628, 2011, 2178, 6251, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "max_length_test = 20\n",
    "test_sentence = 'Test tokenization sentence. Followed by another sentence'\n",
    "\n",
    "# add special tokens\n",
    "\n",
    "test_sentence_with_special_tokens = '[CLS]' + test_sentence + '[SEP]'\n",
    "\n",
    "tokenized = tokenizer.tokenize(test_sentence_with_special_tokens)\n",
    "\n",
    "print('tokenized', tokenized)\n",
    "\n",
    "# convert tokens to ids in WordPiece\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "\n",
    "# precalculation of pad length, so that we can reuse it later on\n",
    "padding_length = max_length_test - len(input_ids)\n",
    "\n",
    "# map tokens to WordPiece dictionary and add pad token for those text shorter than our max length\n",
    "input_ids = input_ids + ([0] * padding_length)\n",
    "\n",
    "# attention should focus just on sequence with non padded tokens\n",
    "attention_mask = [1] * len(input_ids)\n",
    "\n",
    "# do not focus attention on padded tokens\n",
    "attention_mask = attention_mask + ([0] * padding_length)\n",
    "\n",
    "# token types, needed for example for question answering, for our purpose we will just set 0 as we have just one sequence\n",
    "token_type_ids = [0] * max_length_test\n",
    "\n",
    "bert_input = {\n",
    "    \"token_ids\": input_ids,\n",
    "    \"token_type_ids\": token_type_ids,\n",
    "    \"attention_mask\": attention_mask\n",
    "} \n",
    "print(bert_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_append_dispatcher() missing 1 required positional argument: 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-05bd1545b832>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtotal_complaints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtotal_complaints\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m bert_input = tokenizer.encode_plus(\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: _append_dispatcher() missing 1 required positional argument: 'values'"
     ]
    }
   ],
   "source": [
    "total_complaints = np.append(dataset['review'].values)\n",
    "MAX_SEQUENCE_LENGTH = max([len(c.split()) for c in total_complaints])\n",
    "MAX_SEQUENCE_LENGTH\n",
    "\n",
    "bert_input = tokenizer.encode_plus(\n",
    "                        dataset['review'].values,                      \n",
    "                        add_special_tokens = True, # add [CLS], [SEP]\n",
    "                        max_length = MAX_SEQUENCE_LENGTH, # max length of the text that can go to BERT\n",
    "                        pad_to_max_length = True, # add [PAD] tokens\n",
    "                        return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "              )\n",
    "\n",
    "print('encoded', bert_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msZ9sC9eo8RE"
   },
   "source": [
    "# Prepare Train, Validation and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "AcoddPTUZ6Jj",
    "outputId": "9c566f58-cd9e-4105-e37f-897463be49d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 2), (5000, 2), (31806, 2))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = dataset.iloc[:10000]\n",
    "val_df = dataset.iloc[30000:35000]\n",
    "test_df = dataset.iloc[35000:]\n",
    "\n",
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "tfTo2g1vZ6Jm"
   },
   "outputs": [],
   "source": [
    "train_text = train_df['review'].tolist()\n",
    "train_labels = train_df['sentiment'].tolist()\n",
    "\n",
    "val_text = val_df['review'].tolist()\n",
    "val_labels = val_df['sentiment'].tolist()\n",
    "\n",
    "test_text = test_df['review'].tolist()\n",
    "test_labels = test_df['sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClxtbLbYpAiA"
   },
   "source": [
    "# Minimal Text Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "tj4d4rSQZ6Jr",
    "outputId": "5ede03eb-39dc-4e4e-dfec-25342522d1df"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 11689.71it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 11259.39it/s]\n",
      "100%|██████████| 31806/31806 [00:02<00:00, 11211.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import tqdm\n",
    "import re\n",
    "\n",
    "def strip_html_tags(text):\n",
    "  soup = BeautifulSoup(text, \"html.parser\")\n",
    "  [s.extract() for s in soup(['iframe', 'script'])]\n",
    "  stripped_text = soup.get_text()\n",
    "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "  return stripped_text\n",
    "\n",
    "def clean_docs(docs):\n",
    "  norm_docs = []\n",
    "  for doc in tqdm.tqdm(docs):\n",
    "    doc = strip_html_tags(doc)\n",
    "    norm_docs.append(doc)\n",
    "  return norm_docs\n",
    "\n",
    "train_text = clean_docs(train_text)\n",
    "val_text = clean_docs(val_text)\n",
    "test_text = clean_docs(test_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUyYV-iIpD21"
   },
   "source": [
    "# BERT Data Preparation\n",
    "\n",
    "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create InputExample's based on the constructor provided in the BERT library (we model based on that).\n",
    "\n",
    "- `text_a` is the text we want to classify, which in this case, is the review field in our Dataframe.\n",
    "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
    "- `label` is the label for our example, i.e. 1, 0\n",
    "\n",
    "\n",
    "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things.\n",
    "\n",
    "- Lowercase our text (if we're using a BERT lowercase model)\n",
    "- Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "- Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "- Map our words to indexes using a vocab file that BERT provides\n",
    "- Add special \"CLS\" and \"SEP\" tokens (see the readme)\n",
    "- Append \"index\" and \"segment\" tokens to each input (see the BERT paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCmYB8-OrKDR"
   },
   "source": [
    "## BERT InputExamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "MZ-eiTcDZ6Jw"
   },
   "outputs": [],
   "source": [
    "class PaddingInputExample(object):\n",
    "  pass\n",
    "    \n",
    "    \n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YY5IgKlq0Kq"
   },
   "source": [
    "# BERT Tokenization\n",
    "\n",
    "The BERT model we're using expects lowercase data (that's what stored in the tokenization_info parameter do_lower_case. Besides this, we also loaded BERT's vocab file. Finally, we created a tokenizer, which breaks words into word pieces.\n",
    "\n",
    "Word Piece Tokenizer is based on [Byte Pair Encodings (BPE)](https://www.aclweb.org/anthology/P16-1162).\n",
    "\n",
    "WordPiece and BPE are two similar and commonly used techniques to segment words into subword-level in NLP tasks. In both cases, the vocabulary is initialized with all the individual characters in the language, and then the most frequent/likely combinations of the symbols in the vocabulary are iteratively added to the vocabulary.\n",
    "\n",
    "magine that the model sees the word walking. Unless this word occurs at least a few times in the training corpus, the model can't learn to deal with this word very well. However, it may have the words walked, walker, walks, each occurring only a few times. Without subword segmentation, all these words are treated as completely different words by the model.\n",
    "\n",
    "However, if these get segmented as walk@@ ing, walk@@ ed, etc., notice that all of them will now have walk@@ in common, which will occur much frequently while training, and the model might be able to learn more about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "fe--ekRZZ6Jy"
   },
   "outputs": [],
   "source": [
    "def create_tokenizer_from_hub_module(bert_path):\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module =  tf_hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [\n",
    "            tokenization_info[\"vocab_file\"],\n",
    "            tokenization_info[\"do_lower_case\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heH4-emIshLI"
   },
   "source": [
    "# BERT Input Feature Extractor\n",
    "\n",
    "Follows the InputExample instance format of converting each text into:\n",
    "- guid\n",
    "- text_a\n",
    "- text_b\n",
    "- label (optional)\n",
    "\n",
    "![](https://i.imgur.com/sY0xQih.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "wZA98b55Z6J0"
   },
   "outputs": [],
   "source": [
    "def convert_text_to_examples(texts, labels):\n",
    "    \"\"\"Create InputExamples\"\"\"\n",
    "    InputExamples = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text_a=text, text_b=None, label=label)\n",
    "        )\n",
    "    return InputExamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "0bQkkTkzZ6J3"
   },
   "outputs": [],
   "source": [
    "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        input_ids = [0] * max_seq_length\n",
    "        input_mask = [0] * max_seq_length\n",
    "        segment_ids = [0] * max_seq_length\n",
    "        label = 0\n",
    "        return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, example.label\n",
    "\n",
    "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "    for example in tqdm.tqdm(examples, desc=\"Converting examples to features\"):\n",
    "        input_id, input_mask, segment_id, label = convert_single_example(\n",
    "            tokenizer, example, max_seq_length\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "        np.array(labels).reshape(-1, 1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lRGgUhAuYyp"
   },
   "source": [
    "# Loading BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "4fJ4QsIIZ6J5"
   },
   "outputs": [],
   "source": [
    "# Initialize session\n",
    "# sess = tf.Session()\n",
    "\n",
    "# Params for bert model and tokenization\n",
    "BERT_PATH = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "MAX_SEQ_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "fDycqOdjZ6J8",
    "outputId": "150278f6-68ed-465d-b399-2a9aa4a17d5d"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Exporting/importing meta graphs is not supported when eager execution is enabled. No graph exists when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-98f549a215fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Instantiate tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tokenizer_from_hub_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBERT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-47-90448f3d4761>\u001b[0m in \u001b[0;36mcreate_tokenizer_from_hub_module\u001b[0;34m(bert_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_tokenizer_from_hub_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mbert_module\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtf_hub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtokenization_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tokenization_info\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     vocab_file, do_lower_case = sess.run(\n",
      "\u001b[0;32m/opt/app-root/lib64/python3.9/site-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spec, trainable, name, tags)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_parent_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       self._impl = self._spec._create_impl(\n\u001b[0m\u001b[1;32m    177\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/lib64/python3.9/site-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m_create_impl\u001b[0;34m(self, name, trainable, tags)\u001b[0m\n\u001b[1;32m    385\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0mmeta_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_saved_model_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m     return _ModuleImpl(\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0mspec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0mmeta_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/lib64/python3.9/site-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spec, meta_graph, trainable, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;31m# TPU training code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mscope_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_init_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/lib64/python3.9/site-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m_init_state\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_init_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mvariable_tensor_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_state_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     self._variable_map = recover_partitioned_variable_map(\n\u001b[1;32m    456\u001b[0m         get_node_map_from_tensor_map(variable_tensor_map))\n",
      "\u001b[0;32m/opt/app-root/lib64/python3.9/site-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m_create_state_graph\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    507\u001b[0m                                                  absolute_scope_name)\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m     tf.compat.v1.train.import_meta_graph(\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0mmeta_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0minput_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/lib64/python3.9/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1463\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mend_compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m   \"\"\"  # pylint: disable=g-doc-exception\n\u001b[0;32m-> 1465\u001b[0;31m   return _import_meta_graph_with_return_elements(meta_graph_or_file,\n\u001b[0m\u001b[1;32m   1466\u001b[0m                                                  \u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m                                                  **kwargs)[0]\n",
      "\u001b[0;32m/opt/app-root/lib64/python3.9/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_import_meta_graph_with_return_elements\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)\u001b[0m\n\u001b[1;32m   1475\u001b[0m   \u001b[0;34m\"\"\"Import MetaGraph, and return both a saver and returned elements.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1477\u001b[0;31m     raise RuntimeError(\"Exporting/importing meta graphs is not supported when \"\n\u001b[0m\u001b[1;32m   1478\u001b[0m                        \u001b[0;34m\"eager execution is enabled. No graph exists when eager \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                        \"execution is enabled.\")\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exporting/importing meta graphs is not supported when eager execution is enabled. No graph exists when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "# Instantiate tokenizer\n",
    "tokenizer = create_tokenizer_from_hub_module(bert_path=BERT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_VnLNBWuk8J"
   },
   "source": [
    "# Convert Text Data to BERT Input Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMF8qQ7fZ6J-"
   },
   "outputs": [],
   "source": [
    "# Convert data to InputExample format\n",
    "train_examples = convert_text_to_examples(train_text, train_labels)\n",
    "val_examples = convert_text_to_examples(val_text, val_labels)\n",
    "test_examples = convert_text_to_examples(test_text, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "8h1sNHWUZ6KB",
    "outputId": "bbf1f884-2d73-4262-a0a8-44f3a1f8b965"
   },
   "outputs": [],
   "source": [
    "(train_input_ids, train_input_masks, \n",
    " train_segment_ids, train_labels) =  convert_examples_to_features(tokenizer=tokenizer, \n",
    "                                                                  examples=train_examples, \n",
    "                                                                  max_seq_length=MAX_SEQ_LENGTH)\n",
    "\n",
    "(val_input_ids, val_input_masks, \n",
    " val_segment_ids, val_labels) =  convert_examples_to_features(tokenizer=tokenizer, \n",
    "                                                              examples=val_examples, \n",
    "                                                              max_seq_length=MAX_SEQ_LENGTH)\n",
    "\n",
    "(test_input_ids, test_input_masks, \n",
    " test_segment_ids, test_labels) =  convert_examples_to_features(tokenizer=tokenizer, \n",
    "                                                                examples=test_examples, \n",
    "                                                                max_seq_length=MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "uxUvqAcWZ6KH",
    "outputId": "50ea38cc-5c00-4b22-ea03-afa66990cb5f"
   },
   "outputs": [],
   "source": [
    "train_input_ids.shape, val_input_ids.shape, test_input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfuufEMKupyX"
   },
   "source": [
    "# Load BERT Model\n",
    "\n",
    "![](https://i.imgur.com/84Din4P.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "YZZkKQZsZ6KP",
    "outputId": "98182079-bbe4-4b8a-d5ef-84dda7a22e78"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Exporting/importing meta graphs is not supported when eager execution is enabled. No graph exists when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-2ba2f2a1829c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_hub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBERT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"bert_module\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/lib64/python3.9/site-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spec, trainable, name, tags)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_parent_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       self._impl = self._spec._create_impl(\n\u001b[0m\u001b[1;32m    177\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/lib64/python3.9/site-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m_create_impl\u001b[0;34m(self, name, trainable, tags)\u001b[0m\n\u001b[1;32m    385\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0mmeta_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_saved_model_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m     return _ModuleImpl(\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0mspec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0mmeta_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/lib64/python3.9/site-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spec, meta_graph, trainable, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;31m# TPU training code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mscope_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_init_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/lib64/python3.9/site-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m_init_state\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_init_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mvariable_tensor_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_state_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     self._variable_map = recover_partitioned_variable_map(\n\u001b[1;32m    456\u001b[0m         get_node_map_from_tensor_map(variable_tensor_map))\n",
      "\u001b[0;32m/opt/app-root/lib64/python3.9/site-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m_create_state_graph\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    507\u001b[0m                                                  absolute_scope_name)\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m     tf.compat.v1.train.import_meta_graph(\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0mmeta_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0minput_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/app-root/lib64/python3.9/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1463\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mend_compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m   \"\"\"  # pylint: disable=g-doc-exception\n\u001b[0;32m-> 1465\u001b[0;31m   return _import_meta_graph_with_return_elements(meta_graph_or_file,\n\u001b[0m\u001b[1;32m   1466\u001b[0m                                                  \u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m                                                  **kwargs)[0]\n",
      "\u001b[0;32m/opt/app-root/lib64/python3.9/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_import_meta_graph_with_return_elements\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)\u001b[0m\n\u001b[1;32m   1475\u001b[0m   \u001b[0;34m\"\"\"Import MetaGraph, and return both a saver and returned elements.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1477\u001b[0;31m     raise RuntimeError(\"Exporting/importing meta graphs is not supported when \"\n\u001b[0m\u001b[1;32m   1478\u001b[0m                        \u001b[0;34m\"eager execution is enabled. No graph exists when eager \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                        \"execution is enabled.\")\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exporting/importing meta graphs is not supported when eager execution is enabled. No graph exists when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "bm = tf_hub.Module(BERT_PATH, trainable=True, name=f\"bert_module\")\n",
    "bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Vd98ZHIOZ6KV",
    "outputId": "52d94408-e168-4bdb-cbe6-10f45b2d1e26"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-70763d026556>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bm' is not defined"
     ]
    }
   ],
   "source": [
    "len(bm.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F89mn-thvJ-_"
   },
   "source": [
    "# Custom BERT Layer to fine-tune BERT Encoder Layers\n",
    "\n",
    "![](https://i.imgur.com/gnIxACX.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YptN2RqZ6Ka"
   },
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, bert_path, n_fine_tune_encoders=10, **kwargs,):\n",
    "        \n",
    "        self.n_fine_tune_encoders = n_fine_tune_encoders\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.bert_path = bert_path\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.bert = tf_hub.Module(self.bert_path,\n",
    "                                  trainable=self.trainable, \n",
    "                                  name=f\"{self.name}_module\")\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        trainable_vars = [var for var in trainable_vars \n",
    "                                  if not \"/cls/\" in var.name]\n",
    "        trainable_layers = [\"embeddings\", \"pooler/dense\"]\n",
    "\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_encoders+1):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(10 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [var for var in trainable_vars\n",
    "                                  if any([l in var.name \n",
    "                                              for l in trainable_layers])]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:# and 'encoder/layer' not in var.name:\n",
    "                self._non_trainable_weights.append(var)\n",
    "        print('Trainable layers:', len(self._trainable_weights))\n",
    "        print('Non Trainable layers:', len(self._non_trainable_weights))\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(input_ids=input_ids, \n",
    "                           input_mask=input_mask, \n",
    "                           segment_ids=segment_ids)\n",
    "        \n",
    "        pooled = self.bert(inputs=bert_inputs, \n",
    "                           signature=\"tokens\", \n",
    "                           as_dict=True)[\"pooled_output\"]\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPWKg6zbvctA"
   },
   "source": [
    "# Integrate BERT Model for Downstream Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "en6zMDsNZ6Kd"
   },
   "outputs": [],
   "source": [
    "# Build model\n",
    "def build_model(bert_path, max_seq_length, n_fine_tune_encoders=10): \n",
    "    \n",
    "    inp_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    inp_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    inp_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    bert_inputs = [inp_id, inp_mask, inp_segment]\n",
    "    \n",
    "    bert_output = BertLayer(bert_path=bert_path, \n",
    "                            n_fine_tune_encoders=n_fine_tune_encoders)(bert_inputs)\n",
    "    \n",
    "    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
    "    pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=tf.keras.optimizers.Adam(lr=2e-5), \n",
    "                  metrics=['accuracy'])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TXBWH22viaf"
   },
   "source": [
    "# Build BERT Classifier Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMz7b8evZ6Kf"
   },
   "outputs": [],
   "source": [
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "yR7F0QjIZ6Kh",
    "outputId": "606424c6-ba79-4373-f7f1-a72d3af759be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "id": "z1hHURHlZ6Kj",
    "outputId": "b4c5faec-eede-4ac1-db53-8378389151d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable layers: 199\n",
      "Non Trainable layers: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0806 05:50:59.351067 140647828572032 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
      "W0806 05:51:05.398488 140647828572032 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0806 05:51:05.472497 140647828572032 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = build_model(bert_path=BERT_PATH, max_seq_length=MAX_SEQ_LENGTH, n_fine_tune_encoders=10)\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "4bwjo1wGZ6Kl",
    "outputId": "fda7a1c4-a152-4ea0-b079-974d97703f12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer (BertLayer)          (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      bert_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 110,302,011\n",
      "Trainable params: 109,679,361\n",
      "Non-trainable params: 622,650\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbU4i99HvmFM"
   },
   "source": [
    "# Train BERT Classifier (fine-tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "LRDljOeHZ6Kq",
    "outputId": "f2b18f7c-b796-42ce-a7a7-a8af5d044c09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 819s 164ms/sample - loss: 0.3019 - acc: 0.8696 - val_loss: 0.2122 - val_acc: 0.9136\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 812s 162ms/sample - loss: 0.1035 - acc: 0.9650 - val_loss: 0.2317 - val_acc: 0.9188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fea5dadf198>"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], \n",
    "    train_labels,\n",
    "    validation_data=([val_input_ids, val_input_masks, val_segment_ids], val_labels),\n",
    "    epochs=2,\n",
    "    batch_size=15,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9IDmLROvpzj"
   },
   "source": [
    "# Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "AXHqW3bIZ6Kt",
    "outputId": "53aba0c5-41c5-4add-cb1a-7f1cf30d403f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 661s 44ms/sample\n"
     ]
    }
   ],
   "source": [
    "test_predictions = model.predict(x=[test_input_ids, \n",
    "                                    test_input_masks, \n",
    "                                    test_segment_ids],\n",
    "                                 batch_size=100,\n",
    "                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TS-tSX9_Z6Ky"
   },
   "outputs": [],
   "source": [
    "test_predictions = test_predictions.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5yNarXpZ6K1"
   },
   "outputs": [],
   "source": [
    "test_pred_labels = [1 if prob > 0.5 else 0 for prob in test_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "5qoCPNMVZ6K3",
    "outputId": "cb89dd16-7862-46e7-ef4e-e408d27eebcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      7490\n",
      "           1       0.92      0.92      0.92      7510\n",
      "\n",
      "    accuracy                           0.92     15000\n",
      "   macro avg       0.92      0.92      0.92     15000\n",
      "weighted avg       0.92      0.92      0.92     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true=test_labels, y_pred=test_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "L4dZNemXZ6K6",
    "outputId": "e9ae8db8-9713-420c-945a-557a9a3ffae5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEKCAYAAADticXcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XncV2P+x/HXu1XJEhValBINM4RI\nlrFElrGOZRhb4dfIMrITM4wwzYxtmLFkSUamyNj6IX5NjBlCkSVJiYayREql7b7vz++Pc+KW6v7e\ndZ/7++14P3ucR+dc33POdZ3uu8993Z9znesoIjAzs3yoU+wGmJlZzXFQNzPLEQd1M7MccVA3M8sR\nB3UzsxxxUDczyxEHdTOzHHFQNzPLEQd1M7McqVfsBqzIks8m+1FX+55GrfcsdhOsBJUtnq7VPceS\nz6cWHHPqN2u/2vVlxT11M7McKdmeuplZraooL3YLaoSDupkZQHlZsVtQIxzUzcyAiIpiN6FGOKib\nmQFUOKibmeWHe+pmZjniG6VmZjninrqZWX6ER7+YmeWIb5SameWI0y9mZjniG6VmZjninrqZWY74\nRqmZWY74RqmZWX5EOKduZpYfzqmbmeWI0y9mZjninrqZWY6ULyl2C2qEg7qZGTj9YmaWK06/mJnl\niHvqZmY54qBuZpYf4RulZmY54py6mVmO5CT9UqfYDTAzKwlRUfhSBUnrSxou6R1JEyV1k7SBpGck\nTU7/bpruK0k3SZoi6Q1J21c6z0np/pMlnVTIZTiom5lB0lMvdKnan4GnIqITsC0wEbgYGBURHYFR\n6TbAAUDHdOkN3AogaQPgcqArsBNw+dIfBCvjoG5mBjXWU5e0HvBT4C6AiFgcEbOBQ4HB6W6DgcPS\n9UOBeyMxBlhf0ibAfsAzETErIr4EngH2r+oynFM3MwMoq7GXZGwGzAQGSdoWGAecDWwUER+n+3wC\nbJSutwI+rHT8R2nZispXyj11MzOoVk9dUm9JYystvSudqR6wPXBrRGwHzOfbVEtSVUQAkcVluKdu\nZgbVGv0SEQOBgSv4+CPgo4h4Kd0eThLUP5W0SUR8nKZXPks/nw60qXR867RsOrDnMuXPVtU299TN\nzKDGcuoR8QnwoaQt06LuwNvAY8DSESwnAY+m648BJ6ajYHYG5qRpmpFAD0lN0xukPdKylXJP3cwM\nanqc+lnAEEkNgKlAL5JO9AOSTgGmAUen+z4BHAhMAb5O9yUiZknqD7yS7ndlRMyqqmIHdTMzqNEn\nSiNiPNBlOR91X86+AZyxgvPcDdxdnbod1M3MoCZHvxSVg7qZGUBkMhil1jmom5lBbuZ+cVA3MwMH\ndTOzXPHUu2ZmOVJeXuwW1AgHdTMzcPrFzCxXHNTNzHLEOXUzs/yICo9TNzPLD6dfzMxyxKNfzMxy\nxD11Wx1fzZ3H5X+4iSnv/xcE/S8+m7UaNuTKa//KosWLqVu3Lr85tw8/2WpL5sydx29+fyMfTv+E\nhg3r0//is+nYvh0A9w57hIdGPI0EHdu346pL+tKwYYPiXpzViCnvjmHuvHmUl1dQVlbGzt0OZJtt\ntuKWvwxg7SaNmTbtI0448Uzmzp3HPt135+qr+9GgQX0WL17CxRdfxehn/1PsS1iz5CSo+yUZRTLg\npoHs2nUHHh9yG/8YdDPt27bhulsH0afXsTw06GbOPOU4rrt1EAB33PsAnTq25+HBf+GaS89lwJ+T\nF658OvNzhjz0OMPuvIFH7r2FiooKnhz1r2JeltWwffY9ii479mDnbgcCcPttf6Lfpdew3fb78Mgj\nT3L+eX0A+PyLWRx2eE+2234fTj6lL/cM+nMxm71miih8KWEO6kUwd958xr0+gSMO6gFA/fr1WXed\nJgiYN/9rSP9u0WxDAN774L903X4bANq3bcP0Tz7j81lfAlBWXs6iRYspKytnwcJFNG+2Qe1fkNWa\nLTq251/PjwHg/0Y9z+GHJ8F+/PgJfPzxpwBMmDCJRo3WokED/8ZWLRUVhS8lLPOgLqlRpdc6GTD9\n409puv66XHbNjRx58q/57YCb+HrBQi76dW+uu2UQ3Y/oybV/vYu+v0refLXl5pvxf8+9CMCbb0/i\n408/49OZX7BR82b0POZw9jmyF3sddgLrNGnMrjttX8xLsxoUETz5xN95acyTnHrKcQC8/fa7HHLI\nfgAcecRBtGnd8nvH/fznP+O1195i8eLFtdreNV5FFL6UsEyDuqSDgfHAU+l2Z0mPZVnnmqCsvJyJ\n777HLw47kOF330SjRg25a8iDDHvkCS4661RGPXQPF571P/x2QPIr9KnHH8XcefM5otdZDHloBJ06\ndqBunTrMmTuP0f9+iZHD7uKfj9zLggWLeHzk6CJfndWUPfY6nJ267s9BBx9Pnz492X23rpza+1z6\n/OokXhrzJOusszaLFy/5zjFbbbUFv7+6H33OuKhIrV6DlZcXvpSwrHvqVwA7AbPhm1c8bbainSX1\nljRW0tg77x2acdOKZ+PmzdioeTO22Tr5BabHnrvy9qT3eOypUeyzxy4A7LfXbrw58V0AmqzdmKv6\n9eWhQTfz+8vO5cvZc2jdcmPGjB1Pq002YoOm61G/Xj2679GN8W9NLNp1Wc2aMeMTAGbO/IJHH32S\nHXfszKRJ73HAz35J150PYOiwR5k69YNv9m/VahOGP3gXvU4+m6lTpxWp1WuuqKgoeCllWQf1JREx\nZ5myFf7uEhEDI6JLRHQ59cRjMm5a8TTbsCkbt2jG+//9CIAx416nQ7tNad5sA14Z/yYAL417nbbp\nr9ZfzZ3HkiVJj+yhx0eyw7Zb02TtxmzSojlvTJjEgoULiQheGvc67du2Kc5FWY1q3LgRTZqs/c36\nvvvswYQJk2jePLnPIol+l5zN7QP/BsB6663LY4/eS79Lr+GFF8cWrd1rtJykX7Ie0jhB0i+BupI6\nAr8GXsi4zjVCv76ncdGV17JkSRltWm5M/3592Xv3rgz480DKystp2KABl194FgBTp33IpVffgCQ6\nbLYpV158NgDbbL0l++65K0ef0pe6devQqWMHjjpk/2JeltWQjTZqzvAH7wKgXr26DB36CCOffpaz\nzjyFPn16AvDII09wz+BhAJxxei8279COyy49h8suPQeAAw48lpkzvyhK+9dIOZn7RZHh8BxJjYFL\ngR5p0UjgqohYWNWxSz6bXNo/Dq0oGrXes9hNsBJUtni6Vvcc8688ruCYs/Zvh6x2fVnJuqfeKSIu\nJQnsZmalq6y0b4AWKuugfp2kjYHhwLCIeCvj+szMVk1O0i+Z3iiNiL2AvYCZwO2S3pR0WZZ1mpmt\nkpzcKM384aOI+CQibgJOIxmz/tus6zQzq668DGnMNP0i6UfAL4AjgC+AYcB5WdZpZrZKSrwHXqis\nc+p3kwTy/SJiRsZ1mZmtOgf1qkVEtyzPb2ZWY0r88f9CZRLUJT0QEUdLepPvPkEqICJimyzqNTNb\nVX5H6cqdnf59UEbnNzOrWTkJ6pmMfomIj9PV0yNiWuUFOD2LOs3MVovnUy/IvsspOyDjOs3Mqi8n\n49Szyqn3IemRt5f0RqWP1gH84kQzKz0lHqwLlVVO/X7gSeD3wMWVyudGxKyM6jQzW2VRXtpplUJl\nEtTTOdTnAMcCSGoBrAU0kdQkIv6bRb1mZqvMPfWqpa+zux5oCXwGtAUmAltnWa+ZWXXlZUhj1jdK\nrwJ2Bt6NiM2A7sCYjOs0M6u+nNworY3X2X0B1JFUJyJGA10yrtPMrPoqqrGUsKznfpktqQnwL2CI\npM+A+RnXaWZWbVFW4tG6QFn31A8FFgDnAE8B7wEHZ1ynmVn1uadetYio3CsfnGVdZmarIy83SrMe\n/TKX707oBclQx7HAeRExNcv6zcwKVuI98EJlnX65EbgAaAW0Bs4neTBpKMlc62ZmJSEqouClEJLq\nSnpN0oh0+x5J70sany6d03JJuknSFElvSNq+0jlOkjQ5XU4qpN6sb5QeEhHbVtoeKGl8RFwkqV/G\ndZuZFa7me+pnkzyXs26lsgsiYvgy+x0AdEyXrsCtQFdJGwCXk4wYDGCcpMci4suVVZp1T/1rSUdL\nqpMuRwML08/ykcAys1yIssKXqkhqDfwMuLOAqg8F7o3EGGB9SZsA+wHPRMSsNJA/A+xf1cmyDurH\nASeQPE36abp+vKRGwJkZ121mVrCoKHwpwI3AhXy//391mmK5QVLDtKwV8GGlfT5Ky1ZUvlKZBvWI\nmBoRB0dEs4honq5PiYgFEfHvLOs2M6uWagxplNRb0thKS++lp5F0EPBZRIxbpoZLgE7AjsAGwEVZ\nXEamQV3SFpJGSXor3d5G0mVZ1mlmtiqq01OPiIER0aXSMrDSqXYFDpH0AcmgkL0l3RcRH6cplkXA\nIGCndP/pQJtKx7dOy1ZUvlJZp1/uIPnptAQgIt4Ajsm4TjOzaqup9EtEXBIRrSOiHUm8+2dEHJ/m\nyZEk4DDgrfSQx4AT01EwOwNz0rfHjQR6SGoqqSnQIy1bqaxHvzSOiJeTa/hGAbcZzMxqV5Sr6p1W\nzxBJzQEB44HT0vIngAOBKcDXQC+AiJglqT/wSrrflYW8jyLroP65pA6kI10kHQl8vPJDzMxqX4E3\nQKt3zohngWfT9b1XsE8AZ6zgs7up5jM9WQf1M4CBQCdJ04H3SUbEmJmVlKjIvKdeK7IO6tNJbgiM\nJrnb+xVwEnBlxvWamVVLFj31Ysg6qD8KzAZeBWZkXJeZ2SqLcE+9EK0josonoMzMii0vPfUqhzRK\nWltSnXR9C0mHSKpf4PlfkPST1WqhmVktqChXwUspK6Sn/i9g93Sc5NMkw2t+QWE3PHcDekp6H1hE\nMpQnImKbVWyvmVkmfkg3ShURX0s6BbglIv4oaXyB5z9gNdpmZlZrflBBXVI3kp75KWlZ3UJOHhHT\nVrVhZma1KXIyb2whQb0vyaP+D0fEBEntSYYompnlxg+mpx4RzwHPVdqeCvw6y0aZmdW23A9plPQ4\nK3mRRUQckkmLzMyKoLzER7UUamU99WtrrRVmZkWW+556mnYxM/tB+MHk1CV1BH4PbAWstbQ8Itpn\n2C4zs1qVl9EvhbwkYxDJ263LgL2Ae4H7smyUmVltiwoVvJSyQoJ6o4gYRfIQ0rSIuILkLdlmZrlR\nXlGn4KWUFTJOfVE698tkSWeSTKfbJNtmmZnVrh9S+uVsoDHJ2PQdgBNI5kQ3M8uNilDBSykr5OGj\npe/Hm0f67jwzs7zJ/ZDGpSSNZjkPIa3ofXtmZmuivKRfCsmpn19pfS3gCJKRMJlq1HrPrKuwNdCC\nGc8XuwmWU6WeVilUIemXccsU/UfSyxm1x8ysKEp9VEuhCkm/bFBpsw7JzdL1MmuRmVkR5CT7UlD6\nZRzJ9Yok7fI+386rbmaWCz+Y9Avwo4hYWLlAUsOM2mNmVhR5Gf1SSBLpheWUvVjTDTEzK6aKaiyl\nbGXzqW8MtAIaSdqOJP0CsC7Jw0hmZrkR5KOnvrL0y35AT6A1cB3fBvWvgH7ZNsvMrHaV5ST9srL5\n1AcDgyUdEREP1WKbzMxqXV566oXk1HeQtP7SDUlNJV2VYZvMzGpdXnLqhQT1AyJi9tKNiPgSODC7\nJpmZ1b5ABS+lrJAhjXUlNYyIRQCSGgEe0mhmuVLqPfBCFRLUhwCjJA0iuVnaExicZaPMzGpbeYn3\nwAtVyNwvf5D0OrAPyZOlI4G2WTfMzKw2lfhb6gpWSE8d4FOSgH4UyTQBHg1jZrlSkfeeuqQtgGPT\n5XNgGMl7SveqpbaZmdWaH8KEXu8AzwMHRcQUAEnn1EqrzMxqWV5ulK5sSOPPgY+B0ZLukNQdcvL7\niZnZMiqkgpdStsKgHhGPRMQxQCdgNNAXaCHpVkk9aquBZma1obwaSymr8uGjiJgfEfdHxMEk88C8\nBlyUecvMzGpRhQpfSlmho1+Ab54mHZguZma5kfvRL2ZmPyR5Gf2SjzetmpmtpppKv0haS9LLkl6X\nNEHS79LyzSS9JGmKpGGSGqTlDdPtKenn7Sqd65K0fJKk/Qq5Dgd1MzNqdJbGRcDeEbEt0BnYX9LO\nwB+AGyJic+BLvn3X8ynAl2n5Del+SNoKOAbYGtgfuEVS3aoqd1A3MwPKVfiyMpGYl27WT5cA9gaG\np+WDgcPS9UP5dj6t4UB3SUrLh0bEooh4H5gC7FTVdTiom5lRs/OpS6oraTzwGfAM8B4wOyLK0l0+\nInldKOnfHwKkn88BNqxcvpxjVshB3cyM6gV1Sb0lja209K58rogoj4jOJMPAdyJ53qdWePSLmRlQ\nnVeURkRBQ7sjYrak0UA3YH1J9dLeeGtgerrbdKAN8JGkesB6wBeVypeqfMwKuaduZkbNpV8kNV/6\nCtD0pUL7AhNJnsw/Mt3tJODRdP2xdJv0839GRKTlx6SjYzYDOgIvV3Ud7qmbmVGjj/9vAgxOR6rU\nAR6IiBGS3gaGpu94fg24K93/LuBvkqYAs0hGvBAREyQ9ALwNlAFnRESVzXRQNzOj5h7/j4g3gO2W\nUz6V5YxeiYiFJO+qWN65rgaurk79DupmZuRn6l0HdTMzHNTNzHIlL3O/OKibmVH6U+oWykHdzIzS\nf/lFoRzUzcyAipwkYBzUzczwjVIzs1zJRz/dQd3MDHBP3cwsV8qUj766g7qZGU6/mJnlitMvZmY5\n4iGNZmY5ko+Q7qBuZgY4/WJmlivlOemrO6ibmeGeuplZroR76mZm+eGeutWYKe+OYe68eZSXV1BW\nVsbO3Q5k22235pa/DKDhWg0pKyvjrLP68crY8Rx77OFccP7pSGLe3PmccdYlvPHG28W+BKshX82d\nx+UDbmTK1Gkg0b/fOazVsCH9/3QzXy9YSMtNWvCHyy+kydpr88LLr3LjbYNYsqSM+vXrcd4Zp9B1\nh84APPHMs9xx7zAQtGi2IQN+ewFN11+vyFdX2vIypFERpXkh9Rq0Ks2GZWDKu2Po2u0Avvjiy2/K\nnvzf+/nzTXfw1MjRHLD/3px/Xh+673sU3XbuwsR3JjN79hz2328vfvubc9llt4OL2PratWDG88Vu\nQqb69b+W7bf9MUcesj9LlixhwcJF/E/ffpx/5qnsuN02/GPESKbP+JSzep/IxHensGHTprRoviGT\np37Ar865jH8+eh9lZeXsfehxPDrkdpquvx7X/fUu1lqrIWeccnyxLy8z9Zu1X+1XXPRpd3TBMefW\nDx4o2Vdq1Cl2A2z5IoJ11l0HgHXXW4cZH38KwItjxjJ79hwAxrz0Kq1abVK0NlrNmjtvPuNef4sj\nDt4PgPr167PuOk2Y9uF0unT+CQDddtyeZ577NwA/2mJzWjTfEIDNN2vLwkWLWLx4MZH+WbBwIRHB\nvPlf06LZBsW5qDVIGVHwUsoyTb9IEnAc0D4irpS0KbBxRLycZb1rmojgySf+TkRwxx33ceddQzj3\n/Mt5YsT9/HHAb6hTR+y+x6HfO+7kXsfw1MjRRWixZWH6jE9ouv56XHb19UyaMpWttuzIxX1Po8Nm\nbfnn8y/S/ae78PTo5/nk08+/d+wzz/6brbbcnAYNGgDwm/PP5PAT+tCo0Vq0bd2Ky847vbYvZ42T\nlxulWffUbwG6Acem23OBv65oZ0m9JY2VNLaiYn7GTSsde+x1ODt13Z+DDj6ePn16svtuXflV7xM5\n74Ir2KzDjpx3we+44/brvnPMnnvsQq9ex3JJv2uK1GqraWXl5Ux8dwq/OPxnDL/nrzRqtBZ3/e0B\n+vc7h6H/GMHRJ5/F/K8XUL/+d/tiU6ZO4/pb7ua3F5wFwJKyMoY9/L88OOgvjH50CFt02Iw7//ZA\nMS5pjVJRjaWUZR3Uu0bEGcBCgIj4Emiwop0jYmBEdImILnXqrJ1x00rHjBmfADBz5hc8+uiT7Lhj\nZ0484SgefvgJAIYPf5wdd+z8zf4/+cmPuP22P/HzI05m1qwvl3tOW/Ns3KIZGzVvxjZbdwKgx567\n8fa7U2jftg133HgND9x9MwfuswdtKqXcPvlsJmf36881vzmfTVu3BOCdye8BsGnrlkhiv+67M/5N\n30yvSlTjTynLOqgvkVSXdFoFSc0p/R90tapx40Y0abL2N+v77rMHEyZMYsbHn7LHT7sBsPdeuzF5\nyvsAtGnTkgeH3UHPXmczefLUorXbal6zDTdg4xbNeX/aRwCMGTeeDu025YsvZwNQUVHB7YOHcvRh\nBwLJSJnTL7icvqf1Yvtttv7mPBs1a8Z7H/yXWelxL778Gu3bbVrLV7PmyUtPPeshjTcBDwMtJF0N\nHAlclnGda5SNNmrO8AfvAqBevboMHfoII59+lnmnXcD1119JvXr1WLRwIX36XAjAZZeew4YbNuXm\nm5O0y9IhkJYP/c7pw0W/+yNLypbQpuUm9O93Do89NYqh/xgBwD577MLhP+sBwN8fepwPP5rBbYPu\n57ZB9wMw8MaradF8Q/r0Oo6TzriQevXq0nLjFlx96XlFu6Y1RXmJjgSsrsyHNErqBHQHBIyKiImF\nHPdDGtJohcv7kEZbNTUxpPGXbQ8vOObcP+3hkh3SmPXol5uAoRGxwpujZmaloNRz5YXKOqc+DrhM\n0nuSrpXUJeP6zMxWSV5y6pkG9YgYHBEHAjsCk4A/SJqcZZ1mZquigih4KWW1NffL5kAnoC1QUE7d\nzKw25SX9knVO/Y/A4cB7wDCgf0TMzrJOM7NVkZfRL1n31N8DukXE959rNjMrIaWeVilUJkFdUqeI\neAd4Bdg0nfPlGxHxahb1mpmtqlK/AVqorHrq5wK9geuW81kAe2dUr5nZKnFOfSUione6ekBELKz8\nmaS1sqjTzGx15CX9kvU49RcKLDMzK6qIKHgpZVnl1DcGWgGNJG1HMkUAwLpA4yzqNDNbHeU56aln\nlVPfD+gJtAaur1Q+F+iXUZ1mZqssL+mXrHLqg4HBko6IiIeyqMPMrCaVelqlUFmlX46PiPuAdpLO\nXfbziLh+OYeZmRWNe+ort/S1RU0yOr+ZWY3ykMaViIjb079/l8X5zcxqWk1OEyDpbuAg4LOI+HFa\ndgXwP8DMdLd+EfFE+tklwClAOfDriBiZlu8P/BmoC9wZEQOqqjvTIY2S/ihpXUn1JY2SNFPS8VnW\naWa2Kmp4lsZ7gP2XU35DRHROl6UBfSvgGGDr9JhbJNVNXwX6V+AAYCvg2HTflcp6nHqPiPiK5CfW\nBySzNV6QcZ1mZtVWk0E9Iv4FzCqw6kNJXia0KCLeB6YAO6XLlIiYGhGLgaHpviuVdVBfmt75GfBg\nRMzJuD4zs1VSnYePJPWWNLbS0rvqGgA4U9Ibku6W1DQtawV8WGmfj9KyFZWvVNZBfYSkd4AdgFGS\nmgMLqzjGzKzWVaenHhEDI6JLpWVgAVXcCnQAOgMfs/y5sVZbplPvRsTF6ZzqcyKiXNJ8Cvj1wcys\ntmU9+iUiPl26LukOYES6OR1oU2nX1mkZKylfoaxfklEfOB74qSSA54DbsqzTzGxVlEe2k+9K2iQi\nPk43DwfeStcfA+6XdD3QEugIvEwyvUpHSZuRBPNjgF9WVU/WL8m4FagP3JJun5CWnZpxvWZm1VKT\nT5RK+juwJ9BM0kfA5cCekjqTTD/+AfCrtN4Jkh4A3gbKgDMiojw9z5nASJIhjXdHxIQq687y0VhJ\nr0fEtlWVLU+9Bq3y8SSA1agFM54vdhOsBNVv1l5V77Vy2268S8Ex5/VPXljt+rKS9Y3Sckkdlm5I\nak8yuN7MrKRENf6UsqzTLxcAoyVNTbfbAb0yrtPMrNoqcjKhV9Y99f8At5O8/m9Wuv5ixnWamVWb\ne+qFuRf4Cuifbv8S+BtwVMb1mplVS9ajX2pL1kH9xxFRea6C0ZLezrhOM7Nqc/qlMK9K2nnphqSu\nwNiM6zQzqzanXwqzA/CCpP+m25sCkyS9CUREbJNx/WZmBclLTz3roL68qSfNzEpOqffAC5X13C/T\nsjy/mVlNKY98PEKTdU/dzGyN4BdPm5nliF88bWaWI+6pm5nliEe/mJnliEe/mJnliKcJMDPLEefU\nzcxyxDl1M7MccU/dzCxHPE7dzCxH3FM3M8sRj34xM8sR3yg1M8sRp1/MzHLET5SameWIe+pmZjmS\nl5y68vLTKc8k9Y6IgcVuh5UWf1/Y8tQpdgOsIL2L3QArSf6+sO9xUDczyxEHdTOzHHFQXzM4b2rL\n4+8L+x7fKDUzyxH31M3McsRBfQ0jaX1Jp1fabilpeDHbZLVL0mmSTkzXe0pqWemzOyVtVbzWWbE5\n/bKGkdQOGBERPy5yU6wESHoWOD8ixha7LVYa3FOvYZLaSZoo6Q5JEyQ9LamRpA6SnpI0TtLzkjql\n+3eQNEbSm5KukjQvLW8iaZSkV9PPDk2rGAB0kDRe0p/S+t5KjxkjaetKbXlWUhdJa0u6W9LLkl6r\ndC6rZenX6x1JQ9Lvk+GSGkvqnn5t3ky/Vg3T/QdIelvSG5KuTcuukHS+pCOBLsCQ9PuhUaWv+WmS\n/lSp3p6S/pKuH59+L4yXdLukusX4t7CMRISXGlyAdkAZ0DndfgA4HhgFdEzLugL/TNdHAMem66cB\n89L1esC66XozYAqg9PxvLVPfW+n6OcDv0vVNgEnp+jXA8en6+sC7wNrF/rf6IS7p1yuAXdPtu4HL\ngA+BLdKye4G+wIbAJL79jXr99O8rSHrnAM8CXSqd/1mSQN8cmFKp/ElgN+BHwONA/bT8FuDEYv+7\neKm5xT31bLwfEePT9XEk/5F3AR6UNB64nSToAnQDHkzX7690DgHXSHoD+D+gFbBRFfU+AByZrh8N\nLM219wAuTut+FlgL2LTaV2U15cOI+E+6fh/QneR75t20bDDwU2AOsBC4S9LPga8LrSAiZgJTJe0s\naUOgE/CftK4dgFfS74fuQPsauCYrEZ7QKxuLKq2XkwTj2RHRuRrnOI6kt7VDRCyR9AFJMF6hiJgu\n6QtJ2wC/IOn5Q/ID4oiImFSN+i07y97Imk3SK//uThFlknYiCbxHAmcCe1ejnqEkP9zfAR6OiJAk\nYHBEXLJKLbeS55567fgKeF/SUQBKbJt+NgY4Il0/ptIx6wGfpQF9L6BtWj4XWGcldQ0DLgTWi4g3\n0rKRwFnpf2gkbbe6F2SrZVPTqXe1AAADRElEQVRJ3dL1XwJjgXaSNk/LTgCek9SE5Ov4BElqbdvv\nn2ql3w8PA4cCx5IEeEjSgEdKagEgaQNJbVdwvK2BHNRrz3HAKZJeByaQ/GeDJHd6bppm2ZzkV26A\nIUAXSW8CJ5L0toiIL4D/SHqr8o2wSoaT/HB4oFJZf6A+8IakCem2Fc8k4AxJE4GmwA1AL5L03JtA\nBXAbSbAekX5v/Bs4dznnuge4bemN0sofRMSXwESgbUS8nJa9TZLDfzo97zN8mwq0HPCQxiKT1BhY\nkP5qfAzJTVOPTskpD0m1rDmnXnw7AH9JUyOzgZOL3B4zW4O5p25mliPOqZuZ5YiDuplZjjiom5nl\niIO61ThJ5ekQu7ckPZiO8FnVc+0paUS6foiki1ey73dmsKxGHVdIOn9V22hWShzULQsLIqJzOmxv\nMd8+2Qp88/BVtb/3IuKxiBiwkl3WB6od1M3yxEHdsvY8sHk6O+EkSfcCbwFtJPWQ9GI6E+WD6ROU\nSNo/ncnwVeDnS0+0zEyDG0l6WNLr6bILy8xgme53gaRX0lkOf1fpXJdKelfSv4Eta+1fwyxjHqdu\nmZFUDzgAeCot6gicFBFjJDUjebJxn4iYL+kikidr/wjcQTLHyRSSaQ+W5ybguYg4PJ06tglwMfDj\npXPsSOqR1rkTyfw3j0n6KTCf5KnbziT/B14lmXjNbI3noG5ZaJTOAAhJT/0uoCUwLSLGpOU7A1uR\nTHkA0AB4kWQ2wfcjYjKApPuA3supY2+S6ROIiHJgjqSmy+zTI11eS7ebkAT5dUgmuPo6reOx1bpa\nsxLioG5ZWLDsjJRp4J5fuQh4JiKOXWa/6sxkWRUBv4+I25epo28N1mFWUpxTt2IZA+y6dGZCJW9n\n2oJk4rJ2kjqk+x27guNHAX3SY+tKWo/vz1g4Eji5Uq6+VTo74b+Aw9I3Ba0DHFzD12ZWNA7qVhTp\nSxx6An9PZwt8EegUEQtJ0i3/m94o/WwFpzgb2Cud1XAcsNWyM1hGxNMkLx55Md1vOLBORLxKkqt/\nneSNQK9kdqFmtcxzv5iZ5Yh76mZmOeKgbmaWIw7qZmY54qBuZpYjDupmZjnioG5mliMO6mZmOeKg\nbmaWI/8PJvmu8tOT5SAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "with tf.Session() as session:\n",
    "    cm = tf.confusion_matrix(test_labels, test_pred_labels).eval()\n",
    "\n",
    "LABELS = ['negative', 'positive']\n",
    "sns.heatmap(cm, annot=True, xticklabels=LABELS, yticklabels=LABELS, fmt='g')\n",
    "xl = plt.xlabel(\"Predicted\")\n",
    "yl = plt.ylabel(\"Actuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8koPEuM4Z6Lv"
   },
   "source": [
    "# Take Home - Train and fine-tune BERT on the full training data\n",
    "\n",
    "Here we did see BERT's power by training the model on only 5000 examples. But you can bump up the performance even more by using the full training data.\n",
    "\n",
    "For reference, use [this notebook](https://github.com/fabric8-analytics/openshift-probable-vulnerabilities/blob/master/notebooks/modeling/deep_learning_models/phase2_transformer_models/bert_sentiment_analysis_benchmark_models/BERT%20-%20TF%20Keras%20implementation%20-%20Sentiment%20Analysis-Preprocessed%20Text-Seq512.ipynb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text Classification with BERT - Deep Transfer Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
